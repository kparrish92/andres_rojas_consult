---
title: "Sentence Reading Task Results"
output: pdf_document
---
##  Andrés-Rojas (2022)
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```


```{r, include=FALSE}
library(here)
library(tidyverse)
library(kableExtra)

source(here("scripts", "05_load_data.R"))
```

### Overview

The purpose of this document is to provide a statistical analysis of the data of three participants segmented so far (Andrés Rojas, 2022). 
The primary research question investigated was whether there was a difference in rhotic production (both taps and trills) between a pre-test and post-test. 
The following analysis is for the 3 participants in the sentence reading task only. 
Overall, I took the following steps in the analysis:

1. I imported and tidyed the data. The scripts that I used is available in this github repo under `scripts/01_tidy_srt.R`. In addition to the script to tidy the data, I also wrote a function `run_t_test` to help run paired t-tests per participant that can be used later (and to more or less automate further analysis in this style). This function's source code can be found in this repo under `scripts/00_helpers.R`

2. I ran the scripts and analyzed the data to within this document, please see the results below, and made all analysis available in this Github repository (https://github.com/kparrish92/andres_rojas_consult).

### Results 

Overall, it looks like the training worked well for P3E, but not as much for the other two. 
Table 1 shows the mean duration of taps per participant during the pre and post tests, and Table 2 shows the same information for trills.
As you explained, if training is effective, tap duration should be lower at post test. 
This was reflected in the means of both P1E and P3E, but not P5E.
For trills, an increase in duration would suggest that the training was effective: this was only the case for P3E. 


```{r}
tap_df <- srt %>% 
  filter(!duration == "N/A" & !duration == "N/A /d/") %>% 
  group_by(participant, is_trill, time) %>% 
  filter(is_trill == "tap") %>% 
  summarize(mean = mean(as.numeric(duration))) %>% 
  pivot_wider(names_from = participant, values_from = mean) %>% 
  dplyr::select(time, P1E, P3E, P5E, P6E) %>% 
  arrange(desc(time)) 

tap_df$is_trill <- NULL

tap_df %>% 
  knitr::kable(digits = 2, caption = "Table 1: Average duration of tap productions during pre and post-tests per participant") %>% 
  kable_styling(full_width = FALSE)


```

```{r}

trill_df <- srt %>% 
  filter(!duration == "N/A" & !duration == "N/A /d/") %>% 
  group_by(participant, is_trill, time) %>% 
  filter(is_trill == "trill") %>% 
  summarize(mean = mean(as.numeric(duration))) %>% 
  pivot_wider(names_from = participant, values_from = mean) %>%
  dplyr::select(time, P1E, P3E, P5E, P6E) %>% 
  arrange(desc(time))

trill_df$is_trill <- NULL

trill_df %>% 
  knitr::kable(digits = 2, caption = "Table 2: Average duration of trill productions during pre and post-tests per participant") %>% 
  kable_styling(full_width = FALSE)

```


```{r}
srt_t_test <- read.csv(here("data", "tidy", "srt_t_tests.csv")) 
```

In addition to mean comparisons, I did a series of paired t-tests. 
This test determines whether a mean difference can, within a 95% confidence interval, not be equal to zero by taking into account the variation in the response durations submitted to the model. That is, if there was greater deviation in the overall sample, a mean difference between pre and post tests would more likely be due to chance than if the values were more consistent. 
In general, t-tests are reported using this format in APA: 

*t(df) = t value, p = p value*

Df is degrees of freedom and reflects the total number of tokens -1 per comparison. 
This metric determines the shape sampling distribution. 
The t-value and p-value are typically used to assess statistical "significant", which refers to a non-zero difference between two objects of study in a frequentist framework.
It's actually possible to calculate a p-value given the t-value and degrees of freedom. 
The rules of thumb here are that a p-value of less than .05 is considered to be good evidence that there is a non-zero difference between two objects of interest, and typically corresponds to a t-value of greater than 2.

In addition to this reporting, R also determines the mean difference between two objects of interest and generates a 95% confidence interval. 
This measurement can be helpful to report also, since the t-test alone does not give information about a magnitude of an effect nor its directionality 
In this case, we are of course interested in the direction of the effect, and not just whether there is a difference between groups.

Having this in mind, take a look at the results of the first 3 participants. 
A total of 6 paired t-test were carried out, in which each participant's productions of the tap at pre-test and post-test were compared followed by their trill productions at both times (2 t-tests per participant - one for each segment). 

```{r}
srt_t_test %>%
  filter(tap_or_trill == "tap") %>% 
  select(Participant, df, p_val, t_val, estimate, ci_lo, ci_hi) %>% 
  knitr::kable(digits = 3, caption = "
Table 3: Results of Paired t-tests of taps comparing duration during pre and post tests") %>% 
  kable_styling(full_width = FALSE)

```


First, table 3 shows the results of each t-test in which tap duration production is compared from pre to post test. 
The results suggest that the duration of taps by participant P1E fell by 
`r srt_t_test$estimate[1] %>% round(digits = 2)*-1` (95% CI `r srt_t_test$ci_hi[1] %>% round(digits = 2)*-1` - `r srt_t_test$ci_lo[1] %>% round(digits = 2)*-1`), as determined by a paired t-test (t(`r srt_t_test$df[1]`) = `r srt_t_test$t_val[1] %>% round(digits = 3)`, p < .05).
The results of the same test procedure suggested a decrease of `r srt_t_test$estimate[3] %>% round(digits = 2)*-1`ms (95% CI `r srt_t_test$ci_hi[3] %>% round(digits = 2)*-1` - `r srt_t_test$ci_lo[3] %>% round(digits = 2)*-1`; t(`r srt_t_test$df[3]`) = `r srt_t_test$t_val[3] %>% round(digits = 3)`, p < .005) in participant P3E, while participant P5E actually showed evidence of an increased duration in rhotic production for taps of `r srt_t_test$estimate[5] %>% round(digits = 2)` (95% CI `r srt_t_test$ci_hi[5] %>% round(digits = 2)*-1` - `r srt_t_test$ci_lo[5] %>% round(digits = 2)*-1`), as determined by a paired t-test (t(`r srt_t_test$df[5]`) = `r srt_t_test$t_val[5] %>% round(digits = 3)`, p < .246).
Importantly, the results of the paired t-test for P5E were inconclusive, suggesting that there was no evidence that training was effective for this participant.

```{r}
srt_t_test %>%
  filter(tap_or_trill == "trill") %>% 
  select(Participant, df, p_val, t_val, estimate, ci_lo, ci_hi) %>% 
  knitr::kable(digits = 3, caption = "Table 4: Results of Paired t-tests of trills comparing duration during pre and post tests") %>% 
  kable_styling(full_width = FALSE)

```

Table 4 shows an analogous analysis in trill production in which pre-test duration was compared to post-test duration. P1E duration fell by 
`r srt_t_test$estimate[2] %>% round(digits = 2)*-1` (95% CI `r srt_t_test$ci_hi[2] %>% round(digits = 2)*-1` - `r srt_t_test$ci_lo[2] %>% round(digits = 2)*-1`; t(`r srt_t_test$df[2]`) = `r srt_t_test$t_val[2] %>% round(digits = 3)`, p < .005).
On the other hand, P3E showed evidence of an increase in duration of `r srt_t_test$estimate[4] %>% round(digits = 2)`ms (95% CI `r srt_t_test$ci_lo[4] %>% round(digits = 2)` - `r srt_t_test$ci_hi[4] %>% round(digits = 2)`; t(`r srt_t_test$df[4]`) = `r srt_t_test$t_val[4] %>% round(digits = 3)`, p < .005)  while P5E's trill duration also increased from pre to post test by `r srt_t_test$estimate[6] %>% round(digits = 2)`ms (95% CI `r srt_t_test$ci_hi[6] %>% round(digits = 2)*-1` - `r srt_t_test$ci_lo[6] %>% round(digits = 2)`), as determined by a paired t-test (t(`r srt_t_test$df[6]`) = `r srt_t_test$t_val[6] %>% round(digits = 3)`, p < .1).
Again, P5E did not show evidence of distinct performance in trill production from pre to post tests.

### Plots

Here are a few boxplots by participant.
These plots show the distribution of responses per participant in both taps (Figure 1) and trills (Figure 2) between the pre and post tests. 

**Figure 1: Productions of taps per participant during the pre and post test**

```{r}

level_order <- c('PRE', 'POST')

srt %>% 
  filter(!duration == "N/A" & !duration == "N/A /d/") %>%
  filter(is_trill == "tap") %>% 
  ggplot(aes(x = factor(time, level = level_order), y = as.numeric(duration), color = is_trill)) + geom_boxplot(fill = "deepskyblue4", color = "black", outlier.size = 0) + 
  facet_wrap(~participant) + xlab("Test Time") + ylab("Rhotic Duration (ms)") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "grey79"),
        legend.position = "bottom") + theme(legend.position = "none")  + ggtitle("Taps") + ggsave(here("docs", "figs", "figure1.png"))
```



**Figure 2: Productions of trills per participant during the pre and post test**

```{r}


level_order <- c('PRE', 'POST')

srt %>% 
  filter(!duration == "N/A" & !duration == "N/A /d/") %>%
  filter(is_trill == "trill") %>% 
  ggplot(aes(x = factor(time, level = level_order), y = as.numeric(duration), color = is_trill)) + geom_boxplot(fill = "deepskyblue4", color = "black", outlier.size = 0) + 
  facet_wrap(~participant) + xlab("Test Time") + ylab("Rhotic Duration (ms)") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "grey79"),
        legend.position = "bottom") + theme(legend.position = "none") + ggtitle("Trills") + ggsave(here("docs", "figs", "figure2.png"))
```

### Conclusions 

It looks like there is good evidence that training worked for P3E, while P1E did show a decrease in duration for taps, but not an increase for trills, though this effect was small. 
P5E did not appear to respond to training, as pre and post tests were inconclusive.
Overall, adding more participants should reveal whether there is a group trend and paint a clearer picture in terms of how much variation in the effectiveness of training exists. 

